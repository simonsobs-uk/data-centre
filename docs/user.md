# User guide

```{toctree}
:maxdepth: 2
:hidden:
:caption: Contents

user/pipeline.md
```

This is intended to be read by typical users following our recommended usage. While it is written for general audiences, some pointers specific to NERSC users will be given to adapt their pipelines at NERSC to the SO:UK Data Centre.

We will start by pointing out main differences between NERSC and SO:UK Data Centre:


| Facility                                         |  NERSC                                                                                                                                                    |  SO:UK Data Centre                                                                                                                                                                                                                                                                                   |
|:-------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Kind                                             |  HPC                                                                                                                                                      |  HTC                                                                                                                                                                                                                                                                                                 |
| Type                                             |  homogeneous within a pool                                                                                                                                |  heterogeneous by default                                                                                                                                                                                                                                                                            |
|  workload manager                                |  SLURM                                                                                                                                                    |  HTCondor                                                                                                                                                                                                                                                                                            |
| job classification model                         | QoS such as debug, interactive, regular, premium, etc., classified by the priority and charge factors. They have access to the same software environments | universe such as vanilla, parallel, docker, container, etc., classified by the software environments and how the job is launched. Universes are mutually exclusive and hence a job cannot be configured to multiple universes simultaneously. Interactive job is only available in vanilla universe. |
|  name of login nodes                             |  login nodes                                                                                                                                              |  we have a special login node called `vm77` for now                                                                                                                                                                                                                                                  |
|  name of compute nodes                           |  compute nodes                                                                                                                                            |  worker nodes                                                                                                                                                                                                                                                                                        |
|  home directory                                  |  globally mounted home directory, periodically backed up                                                                                                  |  N/A on worker nodes                                                                                                                                                                                                                                                                                 |
|  archive filesystem                              |  HPSS                                                                                                                                                     |  N/A                                                                                                                                                                                                                                                                                                 |
|  scratch filesystem                              |  parallel distributed file system (LUSTRE), all SSD, purged periodically once per few months                                                              |  local to each worker nodes even in parallel jobs, will not persists after job ends                                                                                                                                                                                                                  |
|  filesystem specific for software distribution   | read-only global common                                                                                                                                   | read-only CVMFS                                                                                                                                                                                                                                                                                      |
|  large storage pool                              |  CFS with a filesystem interface                                                                                                                          |  storage elements w/o a filesystem interface                                                                                                                                                                                                                                                         |
|  job configuration format                        |  SLURM directives within the batch script                                                                                                                 |  ClassAd as a separate, ini-like file                                                                                                                                                                                                                                                                |
|  wallclock time                                  |  required to be specified in job configuration                                                                                                            |  N/A                                                                                                                                                                                                                                                                                                 |
|  jobs sharing same physical nodes                |  can be requested via interactive QoS                                                                                                                     |  it is always shared by default                                                                                                                                                                                                                                                                      |
|  dedicated physical nodes exclusive to the job   |  can be requested via regular QoS                                                                                                                         |  N/A                                                                                                                                                                                                                                                                                                 |
|  multiple nodes                                  |  available by default                                                                                                                                     |  need to specify job to be in parallel universe in ClassAd                                                                                                                                                                                                                                           |
|  priority                                        |  different priorities allowed with different charge factors and limitations                                                                               |  N/A                                                                                                                                                                                                                                                                                                 |
|  fair-share system                               |  NERSC hours within an allocation year                                                                                                                    |  more flexible and no hard limit on quota                                                                                                                                                                                                                                                            |
| MPI support                                      | native                                                                                                                                                    | Parallel universe is not designed around MPI exclusively. Custom wrappers are maintained by us to launch MPI processes within parallel universe.                                                                                                                                                     |
| container support                                | officially supported                                                                                                                                      | officially supported only in docker/container universe, where you cannot requests a job to be both in a container universe and a parallel universe                                                                                                                                                   |  

|  Facility                                          |   NERSC                                                                                                                                                                            |   SO:UK Data Centre                                                                                                                                                                                                                                                                                                                  |
|:---------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  Nature                                            |   HPC                                                                                                                                                                              |   HTC                                                                                                                                                                                                                                                                                                                                |
|  Configuration                                     |   Homogeneous within a pool                                                                                                                                                        |   Heterogeneous by default                                                                                                                                                                                                                                                                                                           |
|  Workload Manager                                  |   SLURM                                                                                                                                                                            |   HTCondor                                                                                                                                                                                                                                                                                                                           |
|  Job Classification Model                          | Different QoS can be selected, such as debug, interactive, regular, premium, etc., categorized by priority and charge factors. They shares exactly the same software environments. | Different "universe" can be selected, like vanilla, parallel, docker, container, etc., based on software environments and job launch methods. Universes are mutually exclusive and hence a job cannot be configured to multiple universes simultaneously. Interactive job is only available in vanilla universe.                     |
|  Login Nodes                                       |   Login nodes reachable via ssh with 2-factor authentication. Passwordless login can be achieved by using `sshproxy` service to create temporary ssh keys.                         | Tentatively, a special login node named `vm77` is reachable via ssh. Users are required to submit ssh keys to maintainer, and is passwordless by default.                                                                                                                                                                            |
|  Compute Node Designation                          |   Compute nodes                                                                                                                                                                    |   Worker nodes                                                                                                                                                                                                                                                                                                                       |
|  Home Directory                                    |   Globally mounted home directory, backed up periodically                                                                                                                          |   Not available on worker nodes                                                                                                                                                                                                                                                                                                      |
|  Archive Filesystem                                |   HPSS                                                                                                                                                                             |   Not available                                                                                                                                                                                                                                                                                                                      |
|  Scratch Filesystem                                |   Parallel distributed file system (LUSTRE) with all SSD. Purged once every few months.                                                                                            |   Local to each worker node. Data doesn't persist post job completion.                                                                                                                                                                                                                                                               |
|  Software Distribution Filesystem                  |  Read-only global common                                                                                                                                                           |  Read-only CVMFS                                                                                                                                                                                                                                                                                                                     |
|  Large Storage Pool                                |   CFS with a filesystem interface                                                                                                                                                  |   Storage elements without a filesystem interface                                                                                                                                                                                                                                                                                    |
|  Job Configuration                                 |   SLURM directives within the batch script                                                                                                                                         |   ClassAd in a separate, ini-like format                                                                                                                                                                                                                                                                                             |
|  Wallclock Time                                    |   Must be specified in job configuration                                                                                                                                           |   Not applicable                                                                                                                                                                                                                                                                                                                     |
|  Sharing Physical Nodes                            |   Requested via interactive QoS                                                                                                                                                    |   Always shared by default                                                                                                                                                                                                                                                                                                           |
|  Exclusive Physical Node Allocation                |   Requested via regular QoS                                                                                                                                                        |   Not applicable                                                                                                                                                                                                                                                                                                                     |
|  Utilizing Multiple Nodes                          |   Available by default                                                                                                                                                             |   Must specify parallel universe in ClassAd                                                                                                                                                                                                                                                                                          |
|  Priority                                          |   Different levels permitted with various charge factors and restrictions                                                                                                          |   Not applicable                                                                                                                                                                                                                                                                                                                     |
|  Fair-Share System                                 | Fixed amount of NERSC hours allocated to be used within an allocation year. Proposal required to request and is renewed on a year-to-year basis.                                   |   More flexible with no strict quota limit                                                                                                                                                                                                                                                                                           |
|  MPI Support                                       |  Native                                                                                                                                                                            |  Parallel universe is not exclusively for MPI. We maintain custom wrappers to start MPI processes within the parallel universe.                                                                                                                                                                                                      |
|  Container Support                                 |  Officially supported                                                                                                                                                              |  Only officially supported in the docker/container universe. Jobs cannot belong to both a container universe and a parallel universe.                                                                                                                                                                                                |  
