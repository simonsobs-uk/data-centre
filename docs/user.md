# User guide

```{toctree}
:maxdepth: 2
:hidden:
:caption: Contents

user/pipeline.md
```

This is intended to be read by typical users following our recommended usage. While it is written for general audiences, some pointers specific to NERSC users will be given to adapt their pipelines at NERSC to the SO:UK Data Centre.

We will start by pointing out main differences between NERSC and SO:UK Data Centre:

| Facility                                         |  NERSC                                                                                                                                                    |  SO:UK Data Centre                                                                                                                                                                                                                                                                                   |
|:-------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Kind                                             |  HPC                                                                                                                                                      |  HTC                                                                                                                                                                                                                                                                                                 |
| Type                                             |  homogeneous within a pool                                                                                                                                |  heterogeneous by default                                                                                                                                                                                                                                                                            |
|  workload manager                                |  SLURM                                                                                                                                                    |  HTCondor                                                                                                                                                                                                                                                                                            |
| job classification model                         | QoS such as debug, interactive, regular, premium, etc., classified by the priority and charge factors. They have access to the same software environments | universe such as vanilla, parallel, docker, container, etc., classified by the software environments and how the job is launched. Universes are mutually exclusive and hence a job cannot be configured to multiple universes simultaneously. Interactive job is only available in vanilla universe. |
|  name of login nodes                             |  login nodes                                                                                                                                              |  we have a special login node called `vm77` for now                                                                                                                                                                                                                                                  |
|  name of compute nodes                           |  compute nodes                                                                                                                                            |  worker nodes                                                                                                                                                                                                                                                                                        |
|  home directory                                  |  globally mounted home directory, periodically backed up                                                                                                  |  N/A on worker nodes                                                                                                                                                                                                                                                                                 |
|  archive filesystem                              |  HPSS                                                                                                                                                     |  N/A                                                                                                                                                                                                                                                                                                 |
|  scratch filesystem                              |  parallel distributed file system (LUSTRE), all SSD, purged periodically once per few months                                                              |  local to each worker nodes even in parallel jobs, will not persists after job ends                                                                                                                                                                                                                  |
|  filesystem specific for software distribution   | read-only global common                                                                                                                                   | read-only CVMFS                                                                                                                                                                                                                                                                                      |
|  large storage pool                              |  CFS with a filesystem interface                                                                                                                          |  storage elements w/o a filesystem interface                                                                                                                                                                                                                                                         |
|  job configuration format                        |  SLURM directives within the batch script                                                                                                                 |  ClassAd as a separate, ini-like file                                                                                                                                                                                                                                                                |
|  wallclock time                                  |  required to be specified in job configuration                                                                                                            |  N/A                                                                                                                                                                                                                                                                                                 |
|  jobs sharing same physical nodes                |  can be requested via interactive QoS                                                                                                                     |  it is always shared by default                                                                                                                                                                                                                                                                      |
|  dedicated physical nodes exclusive to the job   |  can be requested via regular QoS                                                                                                                         |  N/A                                                                                                                                                                                                                                                                                                 |
|  multiple nodes                                  |  available by default                                                                                                                                     |  need to specify job to be in parallel universe in ClassAd                                                                                                                                                                                                                                           |
|  priority                                        |  different priorities allowed with different charge factors and limitations                                                                               |  N/A                                                                                                                                                                                                                                                                                                 |
|  fair-share system                               |  NERSC hours within an allocation year                                                                                                                    |  more flexible and no hard limit on quota                                                                                                                                                                                                                                                            |
| MPI support                                      | native                                                                                                                                                    | Parallel universe is not designed around MPI exclusively. Custom wrappers are maintained by us to launch MPI processes within parallel universe.                                                                                                                                                     |
| container support                                | officially supported                                                                                                                                      | officially supported only in docker/container universe, where you cannot requests a job to be both in a container universe and a parallel universe                                                                                                                                                   |  